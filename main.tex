\documentclass{sig-alternate}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{gensymb}
\usepackage{epsfig}
\usepackage{xcolor}

\begin{document}
\conferenceinfo{CIDR '17}{January 8-11, 2017, Santa Cruz, CA, USA}
\newcommand{\smallitem}[1]{\vspace{1em}\noindent\textbf{#1}}
\newcommand{\smallitembot}{\vspace{1em}\noindent}
\bibliographystyle{abbrv}

\newcommand{\jmh}[1]{{\textcolor{red}{[[#1 -- jmh]]}}}
\newcommand{\vikram}[1]{{\textcolor{blue}{[[#1 --vikram]]}}}
% \newcommand{\jmh}[1]{}



\title{Grounding Big Data with Data Context Services}
\numberofauthors{3}
\author{
Groundlings
}

\maketitle

\begin{abstract}
Ground is an open-source data context service, motivated by our experiences in the Big Data ecosystem. By data context, we refer to all the peripheral information that informs the use of data, going well beyond traditional metadata.
The way we use data has changed both philosophically and practically in the last decade, creating an opportunity for new data context services to foster further innovation in Big Data systems and applications. We provide motivation and design guidelines, present our initial design of a common metamodel and API, and explore the current state of the storage solutions that could serve the needs of a data context service. Throughout, we highlight opportunities for new research and engineering solutions.
\end{abstract}

\section{From Metadata to Context}
The open-source Big Data movement, spearheaded by Apache Hadoop, is often discussed in terms of changes in usage: the ``three V's'' of data being captured and the agile working style typified by ``schema-on-use'' and polyglot programming models.

Much less widely discussed is a profound shift toward a decoupled software architecture. Traditionally, database systems were consumed as monolithic stacks of component functionality, regardless of how well-factored they were under the covers. A DBMS included a consistent storage engine, a metadata catalog, a dataflow engine, a language compiler and optimizer, an execution scheduler/runtime, and facilities for data ingest and queueing. In today's Big Data stack, nearly all of these components are independent and swappable. Moreover, there are multiple choices for most of these components in wide use today. The decoupled architecture is healthy for innovation and specialization, and has been embraced by both developers and customers.

One negative side-effect of this diversity is the lack of an agreed-upon service to register metadata for these components. The only widely-used solution is the Hive Metastore, which serves simple relational schemas---a dead end for Variety. As a result, most ``Data Lake'' projects lack even the most basic information; typically it is not even possible to discover what is being stored. 
For emerging Big Data customers and vendors, this Big Metadata problem is hitting a crisis point.  Two serious problems emerge.

The first is poor productivity.
%The promise of the Data Lake is that a diversity of data can be easily captured, and then harnessed by analysts for value. 
In the absence of metadata, analysts are often unable to discover what data exists, much less how it may have been previously parsed, structured, cleansed and analyzed by peers. ``Tribal knowledge'' in an organization is the standard method for disseminating this information today. Clearly this does not scale, leading to wasted work and lost opportunities for analysts to build on the data and analysis known to others.

The second problem is governance risk. Data management necessarily entails recording governance information: who accesses data, what they do with it, where they put it, and how it gets consumed downstream. 
%In some cases this governance metadata is used to enforce policy (e.g.\ access control for Personally Identifiable Information); in others it is logged to support audits for compliance (e.g.\ in the Basel Committee on Banking Supervision). 
In the absence of a standard place to store and access this information, it is impossible to enforce policies and/or audit behavior. As a result, many administrators marginalize their Big Data stack as a playpen for non-sensitive data.

In our diverse experiences in industry, the authors have seen a pressing need for a common service layer to let Big Data components ``write down'' and share metadata information in a flexible way. The effort in this paper began by addressing that need.

\subsection{Metadata Crisis? Context Opportunity.}
The lack of a Big Data metadata service is not just a crisis; it is also a clean-slate opportunity to rethink design and usage. Storage economics and the simplicity of schema-on-use suggest that the Data Lake movement could go much farther than Data Warehouses in enabling diverse, widely-used central repositories of data.  If we view metadata more broadly, could a unified solution address a much broader set of goals than Data Warehouses? We can explore that notion along three axes:

\smallitem{Data.} A schema-on-use world is inherently relativistic. Data does not have \emph{inherent} structure and meaning; rather, the structure is imposed post hoc---sometimes for general usage, sometimes for a specific task.  This means that the ``description'' of a collection of data depends not only on its original form, but on the (many) ways it is transformed for use over time.  

\smallitem{Code}. Data is transformed by code, which becomes a necessary aspect of data description. Transformation is only one kind of data-centric code. There is also code that produces new data: analysis routines, statistical models, and outbound services like recommenders and ad systems. Data \emph{lineage} is a natural byproduct of code, relating it to data sources and outputs. Code also brings along data of its own: training data, model parameters, configuration files, etc. This is not the data of record in an organization; it is an aspect of the code itself. Code management brings its own requirements, notably information on versions, authors and testing.  
%With code and data versioned over time we can envision robust reproducibility of experiments---a feature of interest in areas including hypothesis testing (e.g. A/B tests) and in scientific verification.  

\smallitem{Usage}. In modern agile analytic environments, iterations of exploration, experimentation and (re)deployment of automated pipelines are daily activity. Ironically, today's Big Data software is not well-designed for enabling analysis of its own use. If people learn by doing, then the tribal knowledge of an analytic organization should be visible in its usage logs. Intelligent analytic software could take great advantage of these logs to augment and accelerate human activity and intelligence. Like all software usage logs, analytics logs are themselves big, diverse data.

\smallitembot
The enhanced functionality needed for this layer of the modern Big Data stack goes well beyond traditional metadata management. We refer to it as \emph{Data Context} services. Context refers to the full gamut of peripheral information that informs your analysis: what data and code do you have, where is it stored, when does it get used, who knows about it, and how does it change over time?  

\section{Ground: Scenarios and Design}
We are building an open-source data context service we call \emph{Ground}---a foundation for the broad data context agenda sketched above. 
The goal of Ground is to serve as a central model, API and repository for capturing the broad context in which data gets used. In designing Ground, our intent is to address practical problems for the Big Data community in the short term, and open up opportunities for research and innovation as well.

\subsection{Scenarios}
To illustrate the benefits of data context, consider the following scenario. It is only modestly futuristic: there are applications that do some of these things today, but could do far more if they had access to broader context.

\jmh{\textbf{Idea 1: Intelligent Business Intelligence.}}  Janet, an analyst at a major bank, believes that customers' social network behavior can help explain their likelihood of closing their account (churning). Evaluating this hypothesis requires a pipeline of stages: data acquisition, transformation and analysis. 
% Intelligent applications are emerging for each of these tasks today, which make suggestions to users. All could benefit from a more comprehensive service for data context, as we illustrate below.

Janet 
% plans to purchase a social media ``firehose'' feed for her analysis. To start, though, she 
begins by downloading a small sample of a social media feed from a free API into the data lake. Her data catalog application searches the context service for previously-defined schemas and data sketches, and notifies her that the data lake has a similar feed from the previous month. 
% Looking at that data, she finds it sufficient for her purposes; no need to pay for another feed. 
She then begins using the application to search the context service for historical data on customer churn: what is available, and who has access to it?  As she explores candidate schemas and data samples, the application looks up usage data in the context service and notifies Janet that one of her colleagues has access to many of these datasets, and has previously joined one of them with weather data. Janet decides to start with that dataset as well, and compare notes with her colleague later on.  
% In this scenario, the catalog application was particularly successful because it had access to broad context: not just a large corpus of raw data, but structured schemas for transformed versions of that data based on prior (schema-on-)use, as well as usage data capturing relationships between users, data and actions.

Once Janet chooses particular datasets to focus on, she begins to prepare them for analysis using a data wrangling application. As is typical, the social media feed is semi-structured and deeply nested; the wrangling application searches the context store for previous scripts on similarly-structured data, and suggests unnesting attributes and pivoting them into tabular form.  Recognizing location data in Janet's dataset, the application then consults a geo-reference data in the context store and makes her aware that many of the GPS locations in her data set are very far from any of the bank's branches, and may be candidates for cleaning. The application looks at type-specific analysis routines in the context store, and suggests to Janet that she can generate new columns derived from the social text using code for entity extraction and sentiment analysis written by her data science colleagues. Based on security information in the context service, Janet is warned that certain attributes of the customer churn data were marked confidential by another analyst, and should perhaps be masked. Finally, when she is ready to join the social media names against the bank customer names, the application uses information learned from previous transformation scripts to identify and standardize the join keys appropriately. 
% While the data wrangling application could have made some of these suggestions based on intrinsic properties of the data being transformed, it benefited substantially from peripheral context on other datasets and scripts. 
% : reference data like geographic distributions of branches, repositories of data science routines, and the analytic context that the datasets being wrangled came from the data lake with security annotations. \emph{Would be nicer to get an analytic context about Janet's behavior.}

Armed with a tidy table of hundreds of columns joined together, Janet opens her chosen BI charting application. She plans to cube the data set along various features of users and social media behavior, assessing churn rates in different categories. Given the richness of her wrangled table, the resulting number of potential charts is enormous. Fortunately her BI tool has automated features to recommend charts of interest. Using training data from many other analysts stored in the context service, the recommender focuses on breaking down the data on the custom, algorithmically-extracted sentiment scores and bank-specific entity features, as well as time and space; it omits customer names and the attributes marked confidential.  Janet notices a subcategory of posts with hate speech, and the BI tool enables her to highlight that category and store annotations on the related customers in the context service.
% Here again the BI tool benefits from broad context: lineage from the data wrangling application identifying algorithmic results, metadata on masking from the catalog tool, and training data on chart selection.
% ; these tools use intrinsic properties of their input data today~\cite{jeffheer}. To work better, they could benefit from the lineage of transformations that created their input---in our case recognizing the presence of social media data at the source, and recommending charts that were chosen for visualizing other outputs of social media datasets.
%If interesting patterns emerge in the data visualizations, the analyst may recommend decisions to the organization: e.g. to deploy customer service representatives to respond on social media, or to have the data science team incorporate social media feeds into more sophisticated predictive models for churn.

All of these tools use data context to provide Janet with assistance. Some features save her time on the task she is directly attempting; others provide her contextual information outside her core task---algorithmic ``tribal wisdom''. While some of these features can be provided by current applications that save and learn from their own metadata, all of them benefit substantially from a broader context that spans across applications. Note the ad-hoc, cyclic dependencies in this ``pipeline'': the catalog tool depends on the schemas and sketches generated by users wrangling data (schema-on-use!), the wrangling tool depends on the catalog tool, the BI tool benefits from the lineage of wrangling scripts and populates annotations that can be surfaced back in the catalog.

% \emph{A number of new applications for data analysts have begun to capture data context and provide assistive intelligence as described above, but they currently have no way to share that information.  Hence the scope of their context is limited to what they see at their inputs. Broadly-adopted data context services are key to expanding the intelligence of these applications, harnessing data and computation to improve analyst productivity.}


\jmh{\textbf{Idea 2: Model training and serving.  Joey to fill in? Or borrow a scenario from Johann's paper.} 
% The previous example focused on relatively simple exploratory data analysis. 
Data context can bring similar benefits to the kinds of predictive services that hardcore data scientists build and deploy live in modern hosted applications.  Large-scale predictive services like recommender systems and driving instructions rely on data scientists and engineers working in agile development cycles.  The services are based on serving results from models; the models themselves are periodically trained off of features extracted from data. Data and features evolve over time. Meanwhile, there should be a virtuous cycle of model training, serving and experimentation.  Want to improve this cycle. Want to be able to run new models on old traces (cite Johann's paper). Want to incorporate new models in debug mode in production. Want to be able to reward staff for improving models. Want to reallocate staff when the benefit of experimenting with the model no longer justifies the effort.}

\jmh{\textbf{Idea 3: A Pragmatic example in existing Hadoop workflows.  LinkedIn or Navigator customer story?}  Maybe take Idea 1 and make it less about assistive features, more about lost lineage across HDFS, Trifacta, Hive and Tableau?}

\subsection{Design Requirements}
In a decoupled architecture of multiple applications and backend services, context serves as a ``narrow waist'': a single point of access for the basic information about data and its usage. However, the use of data context remains an open-ended design opportunity. Hence we were keen to focus on simple decisions today that could enable the success of new services and applications in future. To this end we were guided by Postel's Law from Internet architecture: ``Be conservative in what you do, be liberal in what you accept from others''.  With this theme in hand, we identified four key design requirements for a successful data context service.

\smallitem{Model-Agnostic.} For a data context service to be broadly adopted, it cannot impose opinions on metadata modeling. Data models evolve over time, and essentially never die: modern organizations have to manage everything from COBOL data layouts to RDBMS dumps to XML, JSON, Apache logs and free text. As a result, the context service cannot prescribe how metadata is modeled---each dataset may have different metadata to manage. This is a significant weakness in the Big Data stack today: the Hive Metastore captures fixed features or relational schemas; HDFS captures fixed features of files.  A key challenge in Ground is to design a core metamodel that captures generic information that applies to all data, as well as custom information for different data models, applications, and usage. We explore this issue in Section~\ref{sec:metamodel}.

\smallitem{Immutable.} Data context must be immutable; ``updating'' stored context is tantamount to erasing history. Indeed, Postel's Law essentially dictates that we never discard information, lest somebody ask for it. There are multiple reasons why history is critical. The latest context may not always be the most relevant: we may want to replay scenarios from the past for what-if analysis or debugging, or study how context information (say, success rates of a statistical model) change over time. Prior context may also be important for governance and veracity purposes: we may be asked to audit historical behavior and metadata, or reproduce experimental results published in the past. This simplifies record-keeping, but of course it raises significant engineering challenges.  Mature storage solutions in the Big Data ecosystem do not support immutability and versioning natively. \vikram{I feel like a lot of peoples' first response to this is going to be "What about HDFS?" We may want to rephrase a little bit.} We explore this issue in Section~\ref{sec:storage}.

\smallitem{Scalable.} It is a frequent misconception that metadata is small. In fact, metadata scaling was already a challenge in previous-generation technology. In many Big Data settings, it is reasonable to envision the data context being far larger than the data itself. Usage information is one culprit: the logs from a data service can often outstrip the live data managed by the service. Another is the aforementioned desire for immutability: version history can be substantial. Finally, data lineage can also grow to be extremely large
%, depending on the kind of lineage desired
~\cite{cheney2009provenance}.  Of course it is possible to argue that various forms of context information should be managed as ``real data''.  Our main point here is that the use of a context service will encompass analyses and lookups over that information.  We explore these issues in Section~\ref{sec:storage} as well.

\smallitem{Politically Neutral.}  While not a design requirement per se, we note that any narrow-waist service like data context has to ``be Switzerland to be successful''.  Customers will only adopt a central data context service if they feel no fear of lock-in; application writers will prioritize support for widely-used APIs to maximize the benefit of their efforts. 
% Vendor-centric metadata and governance solutions in this space have traditionally raised concerns on those fronts; this is perhaps one reason why the traditional Master Data Management vendors have not been successful in the Big Data market. 
It is important to note here that \emph{open source is not equivalent to political neutrality}; customers and developers have to believe that the project leadership has strong incentives to behave in the common interest. We return to this point in Section~\ref{sec:discussion}.

\vspace{1em}
\jmh{Closing remark here on how these issues make things very interesting indeed!}

\section{Architecture of Ground}
\label{sec:arch}
Ground is composed of a foundational metamodel and the sub-services that back it up.  The metamodel and its Northbound API are critical: they define the Ground surface and semantics, reflecting the new data practices we wish to capture and enhance.  Metamodel definition and support represents the first code we are writing for Ground, and we expect to continue to maintain and evolve it over time.  Behind the metamodel is the Southbound API to five basic services described below.  

We are currently in the midst of an agile development cycle, building and releasing a Minimum Viable Prototype (MVP) of Ground that we call \emph{Ground Zero}. Where appropriate we describe our goals and status for the MVP separately from the longer-term discussion.  A sketch of the Ground Zero architecture is provided in Figure~\ref{fig:layers}.

\begin{figure*}[th]
\centering
\includegraphics[width=0.75\linewidth]{groundarch.pdf}
\caption{A sketch of the Ground architecture.}
\label{fig:layers}
\end{figure*}

\subsection{The Common Ground Metamodel, In Brief}


We begin with an overview of our metamodel, \emph{Common Ground}.
% , which is described in detail in a companion document~\cite{commonground}; we provide an overview here.  
Common Ground provides a three-layer metamodel, with three intertwined graphs capturing versioning at the \emph{core} of the model, metadata modeling at the intermediate \emph{mantle} level, and usage information at the outer \emph{crust} layer.  The core versioning model supports arbitrary DAGs of versions that can be managed in Ground, or be shadowed from external versioning systems like Git. All metadata in Ground is intrinsically versioned. The mantle modeling layer offers an inclusive graph representation that can capture metadata that is represented in both structured and unstructured forms side-by-side, subsuming standard models like Relational, JSON documents, key-value stores and XML.  Usage logs, principals and lineage DAGs in the crust are the third key aspect of the metamodel, enabling the tracking of lineage across both nodes and edges in the underlying models.  
% The interested reader is referred to the Common Ground design document~\cite{commonground} for more detail.

A prototype implementation of Common Ground has been implemented in Scala and is in active use in a deployment at UC Berkeley managing metadata for a large undergraduate course on Database Systems with 500 students and 10 staff. It is tracking metadata about user identity across three authorization systems at Berkeley, workflows and scripts in Python, SQL, PySpark and Jupyter Notebooks, and code versioned externally at GitHub.  This prototype of the metamodel forms the initial basis of the Ground Zero MVP, and has driven the prototypes of some of the services described next.

\subsection{Key Services}
Ground's functionality is backed by five key sub-services.  It was our goal for the initial Ground Zero prototype to use existing open source solutions where possible.  We expect that some of these solutions will fail, and that innovative research will be required to develop viable technology for these services at scale.

\begin{enumerate}
\item \textbf{Versioned Metadata Storage}.  Ground must work with storage in two senses: it must store versioned metadata reliably, and manage references to externally stored data as well.  Ground must be able to store metadata with the full richness of the Common Ground metamodel, including flexible version management, data modeling and lineage storage.  Ground also needs to reference external data, which can come in arbitrary form, with a wide variety of APIs.  Note that Ground is not the primary interface for accessing the data that is referenced by metadata; Ground is expected to \emph{describe} external data and track it, not serve it.  Hence Ground's most basic requirement for external data is to know how to store a unique ID for each item it tracks, and return that ID to applications that request it.  In an ideal setting, each external data item is versioned, hence each version has a unique ID.  However if the external item is mutable and not versioned, Ground generates \emph{Schr\"{o}dinger versions} lazily: each time we observe an object we assume it changes, and assign it a new version (and version ID).  This is discussed in more detail in the Common Ground design~\cite{commonground}.

\textbf{MVP:} We are currently mapping our metadata model onto a traditional PostgreSQL relational database for storage.  Graphs can be represented as relations, so we manage versions, lineage and data modeling graphs at application level above the relational model.  PostgreSQL is a fairly mature system but is not designed to meet our requirements for latency, scalability and availability. In fact we do not expect any relational solution to work well for our needs over time, as relational databases are not designed for any of the three key individual aspects of the metamodel: versioned data, polyglot data models, or rich lineage.  Unfortunately, we do not expect that solutions in the NoSQL or graph database world will fare well either, though this requires research to validate. Therefore a critical thrust of the Ground research agenda is to understand the weaknesses of existing database systems when faced with these requirements, and design a new database system that is well-suited to emerging metadata workloads. \jmh{Need to call out research hypotheses more clearly.}


\item \textbf{Search, Query, Analyze}.  As noted above, Ground is intended to be permissive in the kind of metadata it stores.  First, it needs to support a least-common denominator of unstructured tags, and provide efficient search over those tags.  To this end it needs an indexing and search component.  Second, Ground also needs to support efficient interactive behavior for applications that are inserting, updating and fetching metadata---and capture any changes in a versioned manner.  This should be provided by the metadata storage service with low latency and high throughput.  Third, we expect that metadata will produce a rich target for analytical workloads: algorithms that study what data exists over time, as well as who, how and why data gets used and gets changed.  Finally, it seems natural that some workloads will need to combine these three classes of queries, perhaps via a federated query layer above them. 

\textbf{MVP:} Initially we are not supporting search or analytic APIs; these will be added as the system evolves.  For interactive query, we can only do as well as our prototype relational metadata store.  For search, we expect that existing solutions like Solr will be sufficient for the foreseeable future; we do not expect metadata tagging and querying to exceed volumes that Solr sees in free-text indexing. For analytics, we intend to leverage Spark and GraphX as we have significant expertise in house.  However, the nature of the analytics to be done here represents a major research opportunity: what might be the value of metadata in a Big Data context, and how could that value be extracted by analytics?  Could a \emph{self-aware} Big Data ecosystem improve itself, or provide valuable insight about its usage to applications and users?  \jmh{Again, call out research.}  Finally, the requirements for a federated query layer and its design are a topic for investigation after we acquire a corpus of metadata and workloads.

\item \textbf{Ingestion: Insertion, Queues and Crawlers}.  Metadata may arrive to be stored in Ground interactively or in batches, and it may come actively (via a ``push'' insertion interface), or passively (via a ``pull'' crawling interface).  Interactive insertion of metadata needs to be supported efficiently by the metadata storage component; batch insertion should make use of queueing services to handle bulk delivery and bursty arrivals.  Passive insertion needs to be handled via a data crawler that can register metadata from external services with Ground, and see if Ground can enrich that metadata via additional software services for file parsing and feature extraction.

\textbf{MVP:}  Currently we are handling ingest solely via simple push insertion APIs that call into our metadata store via SQL.  However we envision integrating open-source solutions like Kafka for queuing, and Gobblin for crawling and data ingest from remote sources.  We are also eager to explore APIs to plug in third-party solutions for  extracting metadata from crawled data; two examples we are OpenCalais (a free automated service for entity extraction) and Trifacta (a commercial, semi-automatic solution for data transformation).  We also recognize that there are boundless R\&D opportunities here, some of which could be part of Ground, many of which should exist as standalone solutions above Ground.  \jmh{another research opening, though more about opportunities above ground.}  We look forward to integration with other research and non-research colleagues here.

\item \textbf{Identity and Authorization Integration}.  Identity management and authorization are a required aspect of the Ground service, but almost certainly one that we want to delegate if we can.  The primary reason not to ``bake in'' authorization is administrative: most organizations already have an authorization service and do not want their metadata service to impose a new one.  More importantly, authorization is a semantic notion with wide flexibility: the authorization policies of a federal defense agency are likely to be wildly different in nature from those of a marketing department doing targeted advertising, or an international consortium of scientists interested in reproducibility.  The flexible design of Ground's metamodel should make it possible for a wide variety of use cases to capture authorization metadata (ownership, auditing, content labeling etc.) and design policy over that metadata.  An open design question is whether Ground needs to \emph{enforce} policy, or merely store it.  Note that there is a subtle set of multidimensional connections between metadata versions and policy versions, particularly when the aurthorization policies of a past time are considered unsafe later---a case where good people may disagree about the virtues of immutability.

\textbf{MVP:}  The current MVP has no support for identity management and authorization.  However our initial use case has us tracking UC Berkeley student IDs, Github identities, UNIX uids from instructional computing, and associations between the three; visibility of things like grading scripts and their outputs will depend on policies regarding these identities.  In the short term we expect to integrate with Google oAuth services as exposed at UC Berkeley next, and to explore the way that policy is specified and possibly enforced in our prototpype environment.  Our longterm roadmap here remains open; we expect a need to collaborate closely with partners in application domains to get further requirements.  \jmh{Possible tie to Raluca's work here, at minimum as an example of a non-standard approach to these issues.}

\item \textbf{Scheduling, Workflow, Reproducibility}. In this domain, it is important to separate specification from execution.  We are committed to ensure that Ground is flexible and rich enough to capture the specification of workflows at many granularities of detail: from black-box executables to workflow graphs to source code.  However, we do not expect Ground to be a universal provider of workflow execution or scheduling; instead we hope to integrate with a variety of schedulers and execution frameworks including on-premises and cloud-hosted approaches.

\textbf{MVP:} We plan to begin by utilizing the scheduling and execution services provided by the Gobblin project, which supports a variety of schedulers including Quartz, Azkaban and Oozie, and execution frameworks including Yarn and Helix.  We plan to look into support for VMs and containers as well.  \jmh{Certainly could paint a research picture here, closer to the Bloom-meets-Kubernetes agenda: how will data-centric workflows be programmed in the future, especially as we look at containers, elastic services, etc?}  maybe also a connection to the Shenker/Jackson work on Declarative Datacenters?
\end{enumerate}
% \begin{figure*}[th]
% \center
% \includegraphics[width=0.7\linewidth]{groundarch.pdf}
% \end{figure*}

% \subsection{Common Ground: A Metamodel}
% \label{sec:metamodel}
% \subsection{Underground Services}

\section{Storage: A Prototype Evaluation}
\label{sec:storage}

An initial focus area in our prototype of Ground has been evaluating the current state of play in storage engines. We layered our data model on top of a number of existing open-source storage systems, namely, Postgres, Apache Cassandra, TitanDB, and Neo4j. We then evaluated the data model in a number of different scenarios. 
\vikram{These subsections need to be renamed. These are just placeholders for the time being.}
\subsection{Evaluation Scenarios}

\smallitem{Relational Metadata} One very common type of metadata is the relational schemas that are stored in the Hive Metastore. As a part of our prototype, we have built a connector that allows Ground to serve as a drop-in replacement for the existing Hive Metastore. One major benefit of using Ground as Hive's relational catalog is that schema versioning naturally falls out of this set up. 

\smallitem{Code Versioning} Another key kind of metadata is code versions. An integral part of tracking data usage effectively is understanding which code versions were used to transform or wrangle data. To that end, we have ingested version history graphs from git repositories into Ground. Combined with versioned 

\smallitem{Log Storage} The current state of log analysis tools often revolves around anomaly detection and error correction. However, logs often contain interesting usage data that is currently not being analyzed or leveraged in any way. As a first step, we took an Apache web server log and extracted user session metadata. We used this session data to detect potentially interesting trends, which we will discuss in more detail in the next section.

\smallitem{HDFS Metadata} Lastly, we have built a pipeline using Gobblin and Apache Kafka that extracts file system metadata from HDFS and writes to a Kafka topic, which is then ingested into Ground. This pipeline notifies us of any new files that are added to HDFS. In addition, we are planning on adding hooks which allow Ground to call out to other services (e.g., a parser or featurizer that will extract additional metadata) that might be interested in new files. 

\subsection{Evaluation Functionality}

Functionality:
\begin{itemize}
\item transitive closure
\item point lookup
\item sessionization and trends?
\end{itemize}

Performance/Scale.

\section{Discussion}
\label{sec:discussion}
\jmh{This may evolve into Research Opportunities or Future Work, but this is a placeholder for things that were postponed in earlier text}

\jmh{Backref to Scalability discussion above, and the question ``are logs data or metadata''?}
LinkedIn WhereHows, FINRA Herd, 
Functionality: well, we've started building out a few things and they went well.  Apiary and Grit.

What about performance? Here we ran into some bottlenecks with the widely-used storage systems in the field.  This merits more attention.

\subsection{Initial Results}
\label{sec:perf}
\section{Challenges for the community}
\label{sec:challenges}
\section{Related Work}
\label{sec:relwork}
Old-school MDM. Cloudera Navigator, Apache Atlas, LinkedIn WhereHows, FINRA Herd, Google Goods, IBM LabBook. Host of research on graph queries/DBs. Immutable DBs: Postgres, Datomic, Pachyderm.io, Noms. ReproZip and Burrito for capturing lineage.
\section{Conclusion}
\label{sec:conclusion}
\bibliography{ground}
\end{document}
