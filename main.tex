\documentclass{sig-alternate}
\usepackage[utf8]{inputenc}
\usepackage{times}

\begin{document}
\conferenceinfo{CIDR '17}{January 8-11, 2017, Santa Cruz, CA, USA}

\title{Grounding Big Data with Data Context Services}
\numberofauthors{3}
\author{
Groundlings
}

\maketitle

\section{Metadata Crisis, Context Opportunity}
The open-source Big Data movement, spearheaded by Apache Hadoop, is often discussed in terms of changes in usage: the ``three V's'' of data being captured, and the agile working style typified by ``schema-on-use'' and polyglot programming models.

Much less widely discussed is a profound shift toward a decoupled software architecture. Traditionally, database systems (however elegantly factored internally) were consumed as monolithic stacks of component functionality, including: a storage engine with consistency guarantees, a dataflow engine, a language compiler and optimizer, an execution scheduler/runtime, and facilities for data ingest and queueing.  By contrast, in the Big Data architectures today, nearly all of these components are swappable. Moreover, the set of alternative implementations for the components is evolving over time. This began at the language and optimization layers (MapReduce, Hive, Pig, etc.) and has been particularly active in recent years at the dataflow engine layer (MapReduce, Impala, Drill, Spark, Flink). The decoupled architecture is healthy for innovation and specialization, and has been embraced by both developers and customers.

One side-effect of this decoupling is that there is no agreed-upon service for even the most basic metadata in Big Data systems. The only widely-used metadata service in this arena is the Hive Metastore, which was designed to serve simple relational schemas---a small piece of the Variety envisioned in Big Data. As a result, most ``Data Lake'' projects lack even the most basic metadata information; typically it is not even possible to discover what is being stored. 

For emerging Big Data customers and the vendors who support them, this Big Metadata problem is hitting a crisis point.  Two major problems emerge:
\begin{itemize}
\item \textbf{Poor Productivity.} The promise of the Data Lake is that a diversity of data can be easily captured, and then harnessed by analysts for value. In the absence of metadata, analysts are often unable to discover what data exists, much less how it may have been previously parsed, structured, cleansed and analyzed by their peers. Many organizations speak of ``tribal knowledge'' as the way that this information gets stored and disseminated among analysts. Clearly this does not scale, leading to wasted work and lost opportunities for analysts to build on the data and analysis known to others.
\item \textbf{Governance Risk.} Data management necessarily entails governing the security of data: who sees it, what they do with it, where they put it, and how it gets consumed downstream. In some cases this governance metadata is used to enforce policy (e.g.\ access control for Personally Identifiable Information); in others it is logged to support audits for compliance (e.g.\ in the Basel Committee on Banking Supervision). In the absence of a standard place to store and access this information, it becomes impossible to track and govern these issues, which can force the Big Data stack to be marginalized as a playpen for non-sensitive data.
\end{itemize}

Clearly there is a pressing need for a common service layer to let Big Data components ``write down'' and share metadata information in a flexible way. Part of the effort described in this paper is aimed at addressing that need.

However, before proceeding to address this pressing need, we stopped to consider the opportunity before us: a clean-slate design in an era of significant change. What are the opportunities that a unified solution could provide?  NOTE: DATA LAKE MAY ACTUALLY DELIVER THE CENTRALIZED WAREHOUSE! JUST WITH EMERGENT STRUCTURE.

\begin{itemize}
\item \textbf{Data cataloging in a schema-on-use world.}  How do people impose schema/structure? See Tye's thing.
\item \textbf{Code and Models}.  Versions of course important. Parameters and training data. Reproducibility.  
\item \textbf{Analytic Exhaust}. People and their Behavior.  Usage is metadata. Logs, longitudinal and what-if replay analyses.
\end{itemize}

We refer to this collection of functionality as \emph{Data Context}: what data and code do you have, where is it stored, when does it get used, who knows about it, how does it change over time?  These are all opportunities to go beyond solving the current crisis, providing for a better future.  KEY: MAKE SIMPLE DECISIONS TODAY THAT LEAVE ROOM FOR INNOVATION TOMORROW.


\end{document}
